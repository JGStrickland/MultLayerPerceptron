{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Network for Classification from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "Author: JGStrickland\n",
    "\n",
    "Date: July 2025\n",
    "\n",
    "Building a Neural Network for Classification from Scratch\n",
    "This notebook demonstrates how to build a neural network for classification tasks using Python and NumPy, without relying on high-level machine learning libraries. It covers fundamental concepts and implementation details of neural networks for classification problems.\n",
    "\n",
    "Steps Covered:\n",
    "\n",
    "- Network Architecture – Designing a feedforward neural network with configurable layers, neurons, and activation functions\n",
    "- Initialization – Implementing proper weight initialization techniques (e.g., Xavier/Glorot initialization)\n",
    "- Forward Propagation – Computing predictions through the network using activation functions (sigmoid, ReLU, softmax)\n",
    "- Loss Calculation – Implementing cross-entropy loss for classification tasks\n",
    "- Backpropagation – Calculating gradients using the chain rule efficiently\n",
    "- Parameter Updates – Applying optimization algorithms (e.g., gradient descent, Adam)\n",
    "- Training Loop – Implementing iterative training with batch processing\n",
    "- Prediction – Making class predictions on new data\n",
    "- Evaluation – Measuring model performance using accuracy, precision, recall, and F1-score\n",
    "- Testing – Validating the model on real-world classification datasets\n",
    "\n",
    "This notebook provides a hands-on introduction to the fundamentals of neural networks and demonstrates how the underlying mathematics of deep learning can be implemented programmatically for classification tasks.\n",
    "\n",
    "The implementation will include a flexible neural network class that can be configured with different architectures, making it suitable for various classification problems including binary and multi-class classification.\n",
    "\n",
    "## Step Zero: Imports\n",
    "\n",
    "---\n",
    "\n",
    "We will use Numpy to handle the matrix multiplication needed in this project. Numpy is preferred over native Python lists because it is much faster, more memory-efficient, and provides built-in support for linear algebra operations like matrix multiplication, transposes, and inverses.\n",
    "\n"
   ],
   "id": "6353af21920ac71d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.734670Z",
     "start_time": "2025-09-04T10:54:12.732295Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "6f0cb3dca6476152",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step One: Designing the Architecture\n",
    "\n",
    "---\n",
    "\n",
    "Here we will begin by creating the relevant classes for our classifier, starting off with a Layer class\n",
    "\n",
    "### Layering up\n",
    "\n",
    "---\n",
    "\n",
    "A neural network consists of multiple layers of interconnected neurons, where each layer transforms its input through a weighted sum, bias addition, and non-linear activation function. Let's examine the Layer class attributes in detail:\n",
    "\n",
    "- weights: Matrix (output_size × input_size) that determines the strength of connections between neurons in adjacent layers. Proper initialization is crucial - we use Xavier/Glorot initialization to maintain stable gradients during training.\n",
    "- biases: Vector (output_size × 1) added to the weighted sum, allowing the network to learn offsets and improve model flexibility. Initialized to zeros to start from a symmetric state.\n",
    "- activation: String specifying the non-linear function applied to the layer's output. Hidden layers typically use ReLU, while the output layer uses softmax for classification probabilities.\n",
    "- input: Stores the input values received by this layer during forward propagation, necessary for computing gradients during backpropagation.\n",
    "- output: Stores the activated output values after applying the activation function, serving as input to the next layer.\n",
    "- z: Stores the intermediate linear transformation (weighted sum plus bias) before applying the activation function, needed for gradient calculations.\n",
    "\n",
    "We will explore each of these attributes later in this notebook.\n",
    "\n",
    "### Connecting the Layers\n",
    "\n",
    "---\n",
    "\n",
    "The NeuralNetworkClassifier class orchestrates the complete network architecture by connecting multiple layers in sequence. Let's examine its attributes:\n",
    "\n",
    "- layers: Ordered list containing all Layer objects that constitute the neural network, typically including input, hidden, and output layers.\n",
    "- input_size: Integer specifying the number of input features to the network.\n",
    "- output_size: Integer specifying the number of output classes for classification.\n",
    "- parameters: Dictionary that consolidates all trainable parameters (weights and biases) from all layers, facilitating efficient parameter updates during optimization.\n",
    "- gradients: Dictionary that stores computed gradients for each parameter during backpropagation, used to update weights and biases during training.\n",
    "- prev_size: Tracks the output dimension of the most recently added layer, ensuring dimensional compatibility when adding subsequent layers. Initialized to the input_size.\n",
    "\n",
    "We will explore each of these attributes later in this notebook."
   ],
   "id": "7e78c7095fc2ddff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.742859Z",
     "start_time": "2025-09-04T10:54:12.739710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    def __init__(self, activation):\n",
    "        self.weights = np.array([])\n",
    "        self.biases = np.array([])\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size"
   ],
   "id": "ada5daff36974925",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating the neurons\n",
    "\n",
    "---\n",
    "\n",
    "Now we have initialised the classes we still need to add some functionality before we move on to the next step. Here we are going to expand on the NeuralNetworkClassifier class to create the layers when the class is initalised. This will leave us with ..."
   ],
   "id": "daa81f15cd3ac36b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.755656Z",
     "start_time": "2025-09-04T10:54:12.752863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers = np.append(self.layers, layer)\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers = np.append(self.layers, output_layer)\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases"
   ],
   "id": "39c25322d124a680",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Two: Initialising the weights\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier/Glorot initialization\n",
    "\n",
    "Xavier/Glorot initialization is a technique for initializing the weights of neural networks that helps to stabilize and accelerate training, especially in deep networks. When weights are initialized with values that are too large or too small, it can lead to:\n",
    "\n",
    "- Vanishing gradients: Gradients become extremely small, slowing or stopping learning\n",
    "- Exploding gradients: Gradients become extremely large, causing unstable training\n",
    "- Saturated activation functions: Neurons get \"stuck\" in flat regions of activation functions\n",
    "\n",
    "Xavier initialization sets the initial weights by drawing them from a distribution with:\n",
    "\n",
    "Zero mean\n",
    "Variance specifically calculated based on the number of input and output neurons\n",
    "The formula for the variance is:\n",
    "\n",
    "$$\n",
    "Variance = \\frac{2}{\\left(n_{input} + n_{output}\\right)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $n_{input}$ = the number of neurons in the previous layer\n",
    "- $n_{output}$ = the number of neurons in the current layer\n",
    "\n",
    "In this step we will also initialise the biases to zeros ....\n",
    "\n"
   ],
   "id": "46feea32dee5d0b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.764689Z",
     "start_time": "2025-09-04T10:54:12.762816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None"
   ],
   "id": "af57392ceba4e9e0",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Three: Forward Propagation\n",
    "\n",
    "---\n",
    "\n",
    "The forward propagation process involves two main mathematical operations at each layer:\n",
    "\n",
    "### Linear Transformation\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "z=W⋅x+b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W$ is the weight matrix of size (output_size × input_size)\n",
    "- $x$ is the input vector of size (input_size × 1)\n",
    "- $b$ is the bias vector of size (output_size × 1)\n",
    "- $z$ is the intermediate result before activation\n"
   ],
   "id": "fa37c82934c1222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.773476Z",
     "start_time": "2025-09-04T10:54:12.771449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.z = np.dot(self.weights, self.input) + self.biases\n",
    "        self.output = self.z\n",
    "        return self.output"
   ],
   "id": "73a2c9cd1254207",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Non-linear Activation:\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "a=f(z)\n",
    "$$\n",
    "\n",
    "Where $f$ is the activation function (ReLU, sigmoid, or softmax)\n",
    "\n",
    "When processing a batch of inputs, the input matrix $X$ has shape (input_size × batch_size), and the matrix multiplication $W \\cdot X$ results in a matrix of size (output_size × batch_size). The bias $b$ is added to each column of this result through broadcasting.\n",
    "\n",
    "The activation functions introduce non-linearity:\n",
    "\n",
    "- ReLU: $f(z) = \\max(0, z)$ - Helps with vanishing gradient problem\n",
    "- Sigmoid: $f(z) = \\frac{1}{1 + e^{-z}}$ - Maps values to (0, 1) range\n",
    "- Softmax: $f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ - Converts logits to probabilities\n",
    "\n",
    "The softmax function includes a numerical stability trick: subtracting the maximum value before exponentiation to prevent overflow while maintaining the same relative probabilities.\n",
    "\n",
    "This forward propagation process transforms the input through each layer sequentially, with the output of one layer serving as the input to the next, ultimately producing class probabilities at the final layer."
   ],
   "id": "dfc1711850668a6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.782536Z",
     "start_time": "2025-09-04T10:54:12.779510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.z = np.dot(self.weights, self.input) + self.biases\n",
    "        self.output = self.apply_activation(self.z)\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def apply_activation(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_z = np.exp(z - np.max(z))  # Numerical stability\n",
    "            return exp_z / np.sum(exp_z, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers = np.append(self.layers, layer)\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers = np.append(self.layers, output_layer)\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input"
   ],
   "id": "75f6ba6262ad8a4b",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Four: Loss Calculation\n",
    "\n",
    "----\n",
    "\n",
    "Loss calculation is a critical step in training neural networks as it measures how well the model's predictions match the true labels. For classification problems, we use Cross-Entropy Loss (also known as log loss), which quantifies the difference between predicted probabilities and actual class distributions.\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "----\n",
    "\n",
    "The cross-entropy loss for a single sample is defined as:\n",
    "\n",
    "$$\n",
    "J = -\\sum^{C}_{i=1}y_{i}\\log\\left(\\hat{y}_{i}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C$ = number of classes\n",
    "- $y_i$ = true label (1 for correct class, 0 for others - one-hot encoded)\n",
    "- $\\hat{y}_i$ = predicted probability for class $i$\n",
    "\n",
    "For a batch of $N$ samples, the average loss is:\n",
    "\n",
    "$$\n",
    "J = - \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i=1}^{C}y_{i}^{\\left(n\\right)}\\log\\left(\\hat{y}_{i}^{\\left(n\\right)}\\right)\n",
    "$$\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "---\n",
    "\n",
    "- Numerical stability: We add a small epsilon to prevent log(0) which would be undefined\n",
    "- Batch processing: We compute the average loss across all samples in the batch\n",
    "- One-hot encoding: True labels must be converted to one-hot format for multi-class classification\n",
    "\n",
    "\n"
   ],
   "id": "3fa925240912248f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.790667Z",
     "start_time": "2025-09-04T10:54:12.788358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers = np.append(self.layers, layer)\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers = np.append(self.layers, output_layer)\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n"
   ],
   "id": "30b1a69660f28e28",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Five: Backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "Backpropagation is the algorithm that allows neural networks to learn from data by calculating how each weight and bias should be adjusted to reduce the loss. It works by propagating the error backwards through the network, using the chain rule from calculus to compute gradients efficiently.\n",
    "\n",
    "To fully understand back propagation we must first understand the chain rule in regards to derivatives and partial derivatives.\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "---\n",
    "\n",
    "Finding the derivative of a function allows yyou to understand how much a function changes when as the variables do.\n",
    "\n",
    "So given the function $f\\left(x\\right)$, the derivative $f'\\left(x\\right)$ tells us how $f\\left(x\\right)$ changes as $x$ changes.\n",
    "\n",
    "### Partial Derivatives\n",
    "\n",
    "---\n",
    "\n",
    "Partial derivatives measure how a function changes when only one of its input variables changes, while keeping all other variables constant. In neural networks, we use partial derivatives to understand how the loss function $J$ changes with respect to each individual parameter (weights and biases).\n",
    "Mathematically, the partial derivative of $J$ with respect to a weight $w$ is denoted as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "This tells us the \"direction and rate\" at which $J$ changes as we make small adjustments to $w$.\n",
    "\n",
    "- If $\\frac{\\partial J}{\\partial w} > 0$, increasing $w$ increases the loss, so we should reduce $w$.\n",
    "- If $\\frac{\\partial J}{\\partial w} < 0$, increasing $w$ decreases the loss, so we should increase $w$.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "---\n",
    "\n",
    "The chain rule is a fundamental calculus principle that allows us to compute derivatives of composite functions. If a variable $z$ depends on $y$, and $y$ depends on $x$, then:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} . \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "### Applying the chain rule to neural networks\n",
    "\n",
    "---\n",
    "\n",
    "In a neural network, we have the following sequence of computations:\n",
    "\n",
    "1. Linear step:\n",
    "\n",
    "$$\n",
    "z^{l} = W^{l}a^{l-1}+b^{l}\n",
    "$$\n",
    "\n",
    "2. Activation step:\n",
    "\n",
    "$$\n",
    "a^{l} = f\\left(z^{l}\\right)\n",
    "$$\n",
    "\n",
    "3. Output step:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a^{L}\n",
    "$$\n",
    "\n",
    "4. Loss function:\n",
    "\n",
    "$$\n",
    "J = Loss\\left(\\hat{y},y\\right)\n",
    "$$\n",
    "\n",
    "So the dependencies look like:\n",
    "\n",
    "$$\n",
    "J \\to \\hat{y} \\to a \\to z \\to W,b\n",
    "$$\n",
    "\n",
    "By the chain rule, we can express the gradient of the loss with respect to he weights $W$ as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial \\hat{y}} . \\frac{\\partial \\hat{y}}{\\partial a} . \\frac{\\partial a}{\\partial z} . \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "The bias chain is shorter because the derivative $\\frac{\\partial z}{\\partial b}$ is always 1, while $\\frac{\\partial z}{\\partial W}$ depends on the inputs and requires carrying more terms from the chain. So can be shortened down too simply:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial z}\n",
    "$$\n",
    "\n",
    "This then tells us exactly how much each weight and bias contributed to the error, and therefore how they should be updated in order to reduce the loss.\n",
    "\n",
    "### Error signals\n",
    "\n",
    "---\n",
    "\n",
    "To make backpropagation efficient, instead of repeatedly applying the chain rule directly to every weight and bias, we define the error term ($\\delta$) for each layer:\n",
    "\n",
    "$$\n",
    "\\delta^{l} = \\frac{\\partial J}{\\partial z^{l}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $z^{l}$ is the weighted input to layer $l$ (before applying the activation function).\n",
    "- $\\delta^{l}$ measures how much the loss would change if we changed the pre-activation of this layer.\n",
    "- Intuitively, it tells us how “wrong” the layer’s outputs are and how strongly they need to be corrected.\n",
    "\n",
    "Instead of computing gradients separately for each weight and bias, we compute $\\delta$ once per neuron per layer. Once $\\delta$ is known, the gradients for weights and biases follows:\n",
    "\n",
    "- Weight gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{l}} = \\delta^{l}\\left(a^{l-1}\\right)^{T}\n",
    "$$\n",
    "\n",
    "This multiplies the error by the activations from the previous layer, telling us how each weight contributed to the error.\n",
    "\n",
    "- Bias gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{l}} = \\delta^{l}\n",
    "$$\n",
    "\n",
    "Since the bias is added directly to $z^{l}$, its gradient is just the error itself.\n",
    "\n",
    "To send the error backward to the previous layer we compute:\n",
    "\n",
    "$$\n",
    "\\delta^{l-1} = \\left(W^{l}\\right)^{T}\\delta^{l}\\odot f'\\left(z^{z-1}\\right)\n",
    "$$\n",
    "\n",
    "- $\\left(W^{l}\\right)^{T}\\delta^{l}$ propagates the next layer's error backward through the weights.\n",
    "- $\\odot f'\\left(z^{z-1}\\right)$ multiplies this error element-wise by the derivative of the activation function for each neuron in the previous layer.\n",
    "- This combination tells us how much each neuron in layer $l-1$ contributed to the error in layer $l$, allowing the network to update all weights and biases efficiently.\n",
    "\n",
    "### Activation Function Derivatives\n",
    "\n",
    "---\n",
    "\n",
    "When applying the chain rule, an important term is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "$$\n",
    "\n",
    "This is the derivative of the activation function, which determines how the gradient flows backward through the network. Different activation functions have different derivatives:\n",
    "\n",
    "- **Sigmoid**:\n",
    "  If $a = \\sigma(z) = \\frac{1}{1+e^{-z}}$, then\n",
    "  $$\n",
    "  \\frac{\\partial a}{\\partial z} = a(1-a).\n",
    "  $$\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "  If $a = \\max(0, z)$, then\n",
    "  $$\n",
    "  \\frac{\\partial a}{\\partial z} =\n",
    "  \\begin{cases}\n",
    "  1 & z > 0 \\\\\n",
    "  0 & z \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Softmax** (with cross-entropy loss):\n",
    "  If $a_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$, then\n",
    "  $$\n",
    "  \\frac{\\partial a_i}{\\partial z_j} = a_i (\\delta_{ij} - a_j),\n",
    "  $$\n",
    "  where $\\delta_{ij}$ is the Kronecker delta.\n",
    "  In practice, when combined with cross-entropy loss, this simplifies to:\n",
    "  $$\n",
    "  \\frac{\\partial J}{\\partial z_i} = a_i - y_i.\n",
    "  $$\n",
    "\n",
    "These derivatives are plugged into the chain rule formulas for $\\frac{\\partial J}{\\partial W}$ and $\\frac{\\partial J}{\\partial b}$, allowing the error signal to propagate backward correctly.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e89ae0044078a180"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.800790Z",
     "start_time": "2025-09-04T10:54:12.796520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers = np.append(self.layers, layer)\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers = np.append(self.layers, output_layer)\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        batch_size = y_true.shape[1]\n",
    "\n",
    "        # Start with output layer gradient (softmax cross-entropy derivative)\n",
    "        dZ = (y_pred - y_true) / batch_size  # Average gradient over batch\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            self.gradients[f'dW{l}'] = np.dot(dZ, layer.input.T)\n",
    "            self.gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if l > 0:  # Compute gradient for previous layer\n",
    "                dA_prev = np.dot(layer.weights.T, dZ)\n",
    "                dZ = dA_prev * self.layers[l - 1].activation_derivative(self.layers[l - 1].z)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.z = np.dot(self.weights, self.input) + self.biases\n",
    "        self.output = self.apply_activation(self.z)\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def apply_activation(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_z = np.exp(z - np.max(z))  # Numerical stability\n",
    "            return exp_z / np.sum(exp_z, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        if self.activation == 'relu':\n",
    "            return (z > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-z))\n",
    "            return s * (1 - s)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Note: Softmax derivative is handled separately in cross-entropy loss\n",
    "            raise ValueError(\"Should not use softmax derivative directly in backprop\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")"
   ],
   "id": "6b428d95089b142e",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Six: Parameter Updates\n",
    "\n",
    "---\n",
    "\n",
    "Parameter updates are the final step in the training process where we adjust the weights and biases based on the gradients computed during backpropagation. This is where the actual learning occurs as we iteratively improve the model's parameters to minimize the loss function.\n",
    "\n",
    "### Gradient Descent Optimization\n",
    "\n",
    "---\n",
    "\n",
    "The most basic optimization algorithm is Gradient Descent, which updates parameters in the direction opposite to the gradient of the loss function:\n",
    "\n",
    "$$\n",
    "W = W - \\eta\\frac{\\partial J}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\eta\\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\eta$ is the learning rate, a hyperparameter that controls the step size\n",
    "- $\\frac{\\partial J}{\\partial W}$ and $\\frac{\\partial J}{\\partial b}$ are the gradients computed during backpropagation\n",
    "\n",
    "The learning rate is crucial:\n",
    "\n",
    "- Too small: Slow convergence, may get stuck in local minima\n",
    "- Too large: May overshoot the minimum, causing divergence\n",
    "\n",
    "In our implementation, we update all parameters using these simple gradient descent rules, iteratively moving toward the minimum of the loss function."
   ],
   "id": "730893ef5866a0d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.809845Z",
     "start_time": "2025-09-04T10:54:12.806385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers[i] = layer\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers[-1] = output_layer\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        batch_size = y_true.shape[1]\n",
    "\n",
    "        # Start with output layer gradient (softmax cross-entropy derivative)\n",
    "        dZ = (y_pred - y_true) / batch_size  # Average gradient over batch\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            self.gradients[f'dW{l}'] = np.dot(dZ, layer.input.T)\n",
    "            self.gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if l > 0:  # Compute gradient for previous layer\n",
    "                dA_prev = np.dot(layer.weights.T, dZ)\n",
    "                dZ = dA_prev * self.layers[l - 1].activation_derivative(self.layers[l - 1].z)\n",
    "\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for key in self.parameters:\n",
    "            if key.startswith('W'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "            elif key.startswith('b'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "\n",
    "        # Update layer parameters to match\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.weights = self.parameters[f'W{i}']\n",
    "            layer.biases = self.parameters[f'b{i}']\n"
   ],
   "id": "e961fdcecd248296",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Seven :Training Loop\n",
    "\n",
    "---\n",
    "\n",
    "The training loop orchestrates the complete learning process by repeatedly executing three key steps: forward propagation, backpropagation, and parameter updates. This iterative process continues for a specified number of epochs or until convergence.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "We implement mini-batch gradient descent, which strikes a balance between:\n",
    "\n",
    "- Batch gradient descent (uses all training data, stable but slow)\n",
    "- Stochastic gradient descent (uses one sample at a time, fast but noisy)\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "- Shuffle the training data to ensure random ordering\n",
    "- Split data into mini-batches of specified size\n",
    "- For each mini-batch:\n",
    "- Perform forward propagation to compute predictions\n",
    "- Calculate loss between predictions and true labels\n",
    "- Perform backpropagation to compute gradients\n",
    "- Update parameters using the computed gradients\n",
    "- Repeat for specified number of epochs\n",
    "\n",
    "This approach provides a good compromise between computational efficiency and convergence stability.\n",
    "\n",
    "### Loss Tracking\n",
    "\n",
    "---\n",
    "\n",
    "We track the loss after each epoch to monitor training progress:\n",
    "\n",
    "- Decreasing loss indicates the model is learning\n",
    "- Plateauing loss may suggest convergence or need for learning rate adjustment\n",
    "- Increasing loss may indicate too high learning rate or other issues"
   ],
   "id": "1482de9fe67ba030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.819598Z",
     "start_time": "2025-09-04T10:54:12.815328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers[i] = layer\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers[-1] = output_layer\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        batch_size = y_true.shape[1]\n",
    "\n",
    "        # Start with output layer gradient (softmax cross-entropy derivative)\n",
    "        dZ = (y_pred - y_true) / batch_size  # Average gradient over batch\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            self.gradients[f'dW{l}'] = np.dot(dZ, layer.input.T)\n",
    "            self.gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if l > 0:  # Compute gradient for previous layer\n",
    "                dA_prev = np.dot(layer.weights.T, dZ)\n",
    "                dZ = dA_prev * self.layers[l - 1].activation_derivative(self.layers[l - 1].z)\n",
    "\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for key in self.parameters:\n",
    "            if key.startswith('W'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "            elif key.startswith('b'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "\n",
    "        # Update layer parameters to match\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.weights = self.parameters[f'W{i}']\n",
    "            layer.biases = self.parameters[f'b{i}']\n",
    "\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, epochs=100, batch_size=32, verbose=True):\n",
    "        n_samples = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Shuffle data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            y_shuffled = y[:, permutation]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[:, i:i + batch_size]\n",
    "                y_batch = y_shuffled[:, i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.update_parameters(learning_rate)\n",
    "\n",
    "            # Record average loss for this epoch\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            # Print progress\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n"
   ],
   "id": "c29044c0e8e06e95",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Eight: Prediction\n",
    "\n",
    "---\n",
    "\n",
    "Once trained, the neural network can make predictions on new data. The prediction process involves:\n",
    "\n",
    "- Class Prediction (predict method): Returns the most likely class for each input sample by selecting the class with the highest probability after forward propagation.\n",
    "- Probability Prediction (predict_proba method): Returns the raw probability scores for each class, providing a measure of prediction confidence.\n",
    "\n",
    "These methods enable both categorical predictions and probabilistic outputs, supporting various evaluation scenarios and decision-making processes.\n"
   ],
   "id": "e15b75247b95fbae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.829890Z",
     "start_time": "2025-09-04T10:54:12.825352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers[i] = layer\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers[-1] = output_layer\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        batch_size = y_true.shape[1]\n",
    "\n",
    "        # Start with output layer gradient (softmax cross-entropy derivative)\n",
    "        dZ = (y_pred - y_true) / batch_size  # Average gradient over batch\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            self.gradients[f'dW{l}'] = np.dot(dZ, layer.input.T)\n",
    "            self.gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if l > 0:  # Compute gradient for previous layer\n",
    "                dA_prev = np.dot(layer.weights.T, dZ)\n",
    "                dZ = dA_prev * self.layers[l - 1].activation_derivative(self.layers[l - 1].z)\n",
    "\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for key in self.parameters:\n",
    "            if key.startswith('W'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "            elif key.startswith('b'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "\n",
    "        # Update layer parameters to match\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.weights = self.parameters[f'W{i}']\n",
    "            layer.biases = self.parameters[f'b{i}']\n",
    "\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, epochs=100, batch_size=32, verbose=True):\n",
    "        n_samples = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Shuffle data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            y_shuffled = y[:, permutation]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[:, i:i + batch_size]\n",
    "                y_batch = y_shuffled[:, i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.update_parameters(learning_rate)\n",
    "\n",
    "            # Record average loss for this epoch\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            # Print progress\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.forward(X)"
   ],
   "id": "4fb3aab9c30e19bf",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Nine: Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "Model evaluation is essential to assess performance on unseen data. The evaluation metrics include:\n",
    "\n",
    "- Accuracy: Proportion of correctly classified samples\n",
    "- Precision: Ratio of true positives to all predicted positives for each class\n",
    "- Recall: Ratio of true positives to all actual positives for each class\n",
    "- F1-Score: Harmonic mean of precision and recall\n",
    "- Macro Averages: Mean of per-class metrics, providing class-balanced performance measures\n",
    "\n",
    "These metrics offer a comprehensive view of model performance across all classes, particularly important for imbalanced datasets."
   ],
   "id": "d5b9818aa95f1075"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.840713Z",
     "start_time": "2025-09-04T10:54:12.835357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, activation, hidden_layers=[64, 32], output_size=2):\n",
    "        total_layers = len(hidden_layers) + 1  # hidden layers + output layer\n",
    "        self.layers = np.empty(total_layers, dtype=object)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Track dimensions for layer connections\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i, layer_size in enumerate(hidden_layers):\n",
    "            layer = Layer(prev_size, layer_size, activation)\n",
    "            self.layers[i] = layer\n",
    "            self.parameters[f'W{i}'] = layer.weights\n",
    "            self.parameters[f'b{i}'] = layer.biases\n",
    "            prev_size = layer_size\n",
    "\n",
    "        # Add output layer (always uses softmax for classification)\n",
    "        output_layer = Layer(prev_size, output_size, 'softmax')\n",
    "        self.layers[-1] = output_layer\n",
    "        self.parameters[f'W{len(hidden_layers)}'] = output_layer.weights\n",
    "        self.parameters[f'b{len(hidden_layers)}'] = output_layer.biases\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate input through all layers.\n",
    "        input = X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true, epsilon=1e-12):\n",
    "        # Ensure numerical stability by clipping predictions\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Compute cross-entropy loss for each sample\n",
    "        # -y_true * log(y_pred) and sum across classes\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred), axis=0)\n",
    "\n",
    "        # Return average loss J across the batch\n",
    "        J = np.mean(sample_losses)\n",
    "        return J\n",
    "\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        batch_size = y_true.shape[1]\n",
    "\n",
    "        # Start with output layer gradient (softmax cross-entropy derivative)\n",
    "        dZ = (y_pred - y_true) / batch_size  # Average gradient over batch\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            self.gradients[f'dW{l}'] = np.dot(dZ, layer.input.T)\n",
    "            self.gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            if l > 0:  # Compute gradient for previous layer\n",
    "                dA_prev = np.dot(layer.weights.T, dZ)\n",
    "                dZ = dA_prev * self.layers[l - 1].activation_derivative(self.layers[l - 1].z)\n",
    "\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for key in self.parameters:\n",
    "            if key.startswith('W'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "            elif key.startswith('b'):\n",
    "                self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
    "\n",
    "        # Update layer parameters to match\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.weights = self.parameters[f'W{i}']\n",
    "            layer.biases = self.parameters[f'b{i}']\n",
    "\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, epochs=100, batch_size=32, verbose=True):\n",
    "        n_samples = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Shuffle data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            y_shuffled = y[:, permutation]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                # Get mini-batch\n",
    "                X_batch = X_shuffled[:, i:i + batch_size]\n",
    "                y_batch = y_shuffled[:, i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(y_pred, y_batch)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.update_parameters(learning_rate)\n",
    "\n",
    "            # Record average loss for this epoch\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            self.loss_history.append(avg_loss)\n",
    "\n",
    "            # Print progress\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Get predictions\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = np.argmax(y, axis=0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred == y_true)\n",
    "\n",
    "        # Calculate precision, recall, and F1 for each class\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "\n",
    "        for class_idx in range(self.output_size):\n",
    "            # True positives, false positives, false negatives\n",
    "            tp = np.sum((y_pred == class_idx) & (y_true == class_idx))\n",
    "            fp = np.sum((y_pred == class_idx) & (y_true != class_idx))\n",
    "            fn = np.sum((y_pred != class_idx) & (y_true == class_idx))\n",
    "\n",
    "            # Avoid division by zero\n",
    "            prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_score = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "            precision.append(prec)\n",
    "            recall.append(rec)\n",
    "            f1.append(f1_score)\n",
    "\n",
    "        # Calculate macro averages\n",
    "        macro_precision = np.mean(precision)\n",
    "        macro_recall = np.mean(recall)\n",
    "        macro_f1 = np.mean(f1)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'macro_precision': macro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'macro_f1': macro_f1\n",
    "        }"
   ],
   "id": "7b551b398522e002",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step Ten: Testing\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset overview\n",
    "\n",
    "---\n",
    "\n",
    "We tested our neural network implementation on the Digits dataset from scikit-learn:\n",
    "- Samples: 1,797 handwritten digit images\n",
    "- Features: 64 (8×8 pixel images flattened to 1D vectors)\n",
    "- Classes: 10 (digits 0-9)\n",
    "- Task: Multi-class classification of handwritten digits\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "---\n",
    "\n",
    "1. Feature Standardization: Pixel values were standardized to have zero mean and unit variance\n",
    "2. Label Encoding: Class labels were one-hot encoded for compatibility with our neural network\n",
    "3. Train-Test Split: Data was divided into 80% training and 20% testing sets\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "---\n",
    "\n",
    "The neural network was configured with:\n",
    "\n",
    "- Input Layer: 64 neurons (matching the flattened image dimensions)\n",
    "- Hidden Layers: Two layers with 64 and 32 neurons respectively\n",
    "- Output Layer: 10 neurons (one for each digit class)\n",
    "- Activation Function: ReLU for hidden layers, Softmax for output layer\n",
    "- Learning Rate: 0.01\n",
    "- Batch Size: 32\n",
    "- Epochs: 500\n"
   ],
   "id": "9ab23484994f1347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:12.847568Z",
     "start_time": "2025-09-04T10:54:12.846189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ],
   "id": "51193d39ccdbb8b4",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:54:14.838590Z",
     "start_time": "2025-09-04T10:54:12.853586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data  # Features: 64 (flattened image pixels)\n",
    "y = digits.target  # Labels: 10 (0-9)\n",
    "\n",
    "# Preprocess the data\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=3\n",
    ")\n",
    "\n",
    "# Transpose to match our model's expected input format (features x samples)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "model = NeuralNetworkClassifier(\n",
    "    input_size=64,  # 64 features (pixels)\n",
    "    activation='relu',  # ReLU activation for hidden layers\n",
    "    hidden_layers=[64, 32],  # Larger hidden layers for this more complex problem\n",
    "    output_size=10  # 10 classes (digits 0-9)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the neural network on Digits dataset...\\n\")\n",
    "model.fit(X_train, y_train, learning_rate=0.01, epochs=500, batch_size=32, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nModel Performance on Test Set:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {[f'{p:.4f}' for p in results['precision']]}\")\n",
    "print(f\"Recall: {[f'{r:.4f}' for r in results['recall']]}\")\n",
    "print(f\"F1-Score: {[f'{f:.4f}' for f in results['f1']]}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "true_labels = np.argmax(y_test, axis=0)\n",
    "\n",
    "print(f\"\\nSample predictions vs true labels:\")\n",
    "for i in range(min(15, len(predictions))):\n",
    "    print(f\"Predicted: {predictions[i]}, True: {true_labels[i]}\")"
   ],
   "id": "2609debcda00d783",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the neural network on Digits dataset...\n",
      "\n",
      "Epoch 0: Loss = 2.3426\n",
      "Epoch 10: Loss = 0.5462\n",
      "Epoch 20: Loss = 0.2288\n",
      "Epoch 30: Loss = 0.1382\n",
      "Epoch 40: Loss = 0.0964\n",
      "Epoch 50: Loss = 0.0716\n",
      "Epoch 60: Loss = 0.0554\n",
      "Epoch 70: Loss = 0.0441\n",
      "Epoch 80: Loss = 0.0359\n",
      "Epoch 90: Loss = 0.0297\n",
      "Epoch 100: Loss = 0.0250\n",
      "Epoch 110: Loss = 0.0213\n",
      "Epoch 120: Loss = 0.0184\n",
      "Epoch 130: Loss = 0.0160\n",
      "Epoch 140: Loss = 0.0141\n",
      "Epoch 150: Loss = 0.0125\n",
      "Epoch 160: Loss = 0.0112\n",
      "Epoch 170: Loss = 0.0101\n",
      "Epoch 180: Loss = 0.0092\n",
      "Epoch 190: Loss = 0.0084\n",
      "Epoch 200: Loss = 0.0078\n",
      "Epoch 210: Loss = 0.0072\n",
      "Epoch 220: Loss = 0.0067\n",
      "Epoch 230: Loss = 0.0062\n",
      "Epoch 240: Loss = 0.0058\n",
      "Epoch 250: Loss = 0.0055\n",
      "Epoch 260: Loss = 0.0051\n",
      "Epoch 270: Loss = 0.0049\n",
      "Epoch 280: Loss = 0.0046\n",
      "Epoch 290: Loss = 0.0044\n",
      "Epoch 300: Loss = 0.0041\n",
      "Epoch 310: Loss = 0.0039\n",
      "Epoch 320: Loss = 0.0038\n",
      "Epoch 330: Loss = 0.0036\n",
      "Epoch 340: Loss = 0.0034\n",
      "Epoch 350: Loss = 0.0033\n",
      "Epoch 360: Loss = 0.0032\n",
      "Epoch 370: Loss = 0.0030\n",
      "Epoch 380: Loss = 0.0029\n",
      "Epoch 390: Loss = 0.0028\n",
      "Epoch 400: Loss = 0.0027\n",
      "Epoch 410: Loss = 0.0026\n",
      "Epoch 420: Loss = 0.0025\n",
      "Epoch 430: Loss = 0.0025\n",
      "Epoch 440: Loss = 0.0024\n",
      "Epoch 450: Loss = 0.0023\n",
      "Epoch 460: Loss = 0.0022\n",
      "Epoch 470: Loss = 0.0022\n",
      "Epoch 480: Loss = 0.0021\n",
      "Epoch 490: Loss = 0.0020\n",
      "Epoch 499: Loss = 0.0020\n",
      "\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.9639\n",
      "Precision: ['1.0000', '0.9459', '0.9677', '0.9750', '0.9762', '0.9714', '1.0000', '0.9767', '0.8788', '0.9310']\n",
      "Recall: ['1.0000', '1.0000', '0.9677', '0.9750', '0.9318', '0.8947', '0.9200', '1.0000', '1.0000', '0.9310']\n",
      "F1-Score: ['1.0000', '0.9722', '0.9677', '0.9750', '0.9535', '0.9315', '0.9583', '0.9882', '0.9355', '0.9310']\n",
      "\n",
      "Sample predictions vs true labels:\n",
      "Predicted: 0, True: 0\n",
      "Predicted: 4, True: 4\n",
      "Predicted: 1, True: 1\n",
      "Predicted: 3, True: 2\n",
      "Predicted: 0, True: 0\n",
      "Predicted: 0, True: 0\n",
      "Predicted: 8, True: 8\n",
      "Predicted: 7, True: 7\n",
      "Predicted: 6, True: 6\n",
      "Predicted: 6, True: 6\n",
      "Predicted: 3, True: 3\n",
      "Predicted: 6, True: 6\n",
      "Predicted: 9, True: 9\n",
      "Predicted: 7, True: 7\n",
      "Predicted: 4, True: 4\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "---\n",
    "\n",
    "This notebook has successfully demonstrated the implementation of a neural network for classification from scratch using only NumPy. The project provides a comprehensive foundation in deep learning fundamentals, covering all critical aspects of neural network design, training, and evaluation.\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "---\n",
    "\n",
    "1. Complete Implementation: Built a functional neural network with configurable architecture, proper weight initialization, forward/backward propagation, and training loop\n",
    "2. Mathematical Foundation: Incorporated proper mathematical formulations for activation functions, loss calculation, and gradient computation\n",
    "3. Practical Application: Successfully tested the implementation on the Digits dataset with strong performance\n",
    "4. Evaluation Metrics: Implemented comprehensive evaluation including accuracy, precision, recall, and F1-score\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "---\n",
    "\n",
    "The neural network achieved competitive performance on the challenging Digits dataset, demonstrating its capability to handle real-world classification problems with multiple classes and features.\n",
    "\n",
    "### Next Steps and Future Improvements\n",
    "\n",
    "---\n",
    "\n",
    "While this implementation provides a solid foundation, several enhancements could further improve its robustness and performance:\n",
    "\n",
    "1. Regularization Techniques\n",
    "\n",
    "- L2 Regularization: Add weight decay to prevent overfitting\n",
    "- Dropout: Implement random neuron deactivation during training\n",
    "- Early Stopping: Monitor validation loss to prevent overtraining\n",
    "\n",
    "2. Advanced Optimization\n",
    "\n",
    "- Adam Optimizer: Implement adaptive learning rate optimization\n",
    "- Learning Rate Scheduling: Add decay or cosine annealing for better convergence\n",
    "- Momentum: Incorporate momentum in gradient updates for smoother optimization\n",
    "\n",
    "3. Model Validation\n",
    "\n",
    "- K-Fold Cross Validation: Implement proper cross-validation for more reliable performance estimates\n",
    "- Validation Set: Separate training data into training/validation splits for hyperparameter tuning\n",
    "- Hyperparameter Optimization: Add grid search or random search for optimal parameter selection\n",
    "\n",
    "4. Architectural Enhancements\n",
    "\n",
    "- Batch Normalization: Implement normalization between layers for faster training\n",
    "- Different Architectures: Support for convolutional layers (CNNs) for image data\n",
    "- Residual Connections: Add skip connections for deeper networks\n",
    "\n",
    "5. Visualization and Analysis\n",
    "\n",
    "- Training Curves: Plot loss and accuracy over epochs\n",
    "- Confusion Matrices: Visualize classification performance\n",
    "- Feature Visualization: Explore what the network has learned\n",
    "\n",
    "6. Additional Datasets\n",
    "\n",
    "- Iris Dataset: Test the model on this classic classification benchmark\n",
    "- More Complex Datasets: Evaluate performance on larger datasets like CIFAR-10 or MNIST\n"
   ],
   "id": "b6fd1ce0c7df6009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27cdb0c8071a9c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
